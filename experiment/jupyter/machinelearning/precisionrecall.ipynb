{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall\n",
    "\n",
    "In machine learning model, we have mentioned that, there is an important concept called metrics. However, for classifications problems, accuracy is one of the metrics. There are other important metrics.\n",
    "\n",
    "In this exercise, we will test our model with new metrics: Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please answer Questions\n",
    "\n",
    "To help you understand precision and recall. Please answer questions below by searching and input your answers.\n",
    "\n",
    "* **Question 1: What is your understanding of these terms: true postive, false postive, true negative, false negative?**\n",
    "\n",
    "    * true positive - a true positive occurs when an outcome/observation is detected and that outcome/observation is present\n",
    "\n",
    "    * true negative - a true negative occurs when an outcome/observation is not detected and that outcome/observation is not present\n",
    "\n",
    "    * false positive - a false positive occurs when an outcome/observation is detected and that outcome/observation is not actually present\n",
    "\n",
    "    * false negative - a false negative occurs when an outcome/observation is not detected and that outcome/observation is actually present\n",
    "\n",
    "* **Question 2: What are the relationships between those terms and precision or recall?**\n",
    "\n",
    "* Please write down your answer by two simple mathematical equation\n",
    "\n",
    "    * precision - precision, also known as positive predictive value, is the proportion of positive predictions that are actually true. this can be represented by the equation:\n",
    "        * true positives / (true positives + false positives)\n",
    "    * recall - recall, also known as sensitivity or true positive rate, is the proportion of positive outcomes/observations that are correctly identified. this can be represented by the equation:\n",
    "        * true positives / (true positives + false negatives)\n",
    "\n",
    "\n",
    "** Answer:** Please double click the cell and input your answer here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import key modules from scikit-learn and import numpy library\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is an example for how to get precision of your model\n",
    "\n",
    "** Attention **: You need to finish one line of code to implement the whole example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load iris data again\n",
    "\n",
    "#import iris dataset, set variables X, y to the independent and dependent variables\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 804)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the data attribute of the iris dataset includes 150 samples, each sample has up to four features\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the data to training and testing data.\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "#the numpy function 'c_' (short for concatenate) combines together two separate numpy arrays\n",
    "#this next line of code takes the original data array held by the variable X and concatenates\n",
    "#another array of random numbers that has the shape (# of samples, 200 * # of features). \n",
    "#In this case we are concatenating an array of random numbers that is 150 by 800\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# Limit to the two first classes, and split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
    "                                                    test_size=.5,\n",
    "                                                    random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple classifier\n",
    "classifier = svm.LinearSVC(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2',\n",
       "     random_state=<mtrand.RandomState object at 0x7f19f7041678>,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How could we fit the model? Please find your solutions from our example, and write down \n",
    "#your code to fit the svm model from training data.\n",
    "\n",
    "#fit using X_train and y_train\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have fit the model, then we make predicions.\n",
    "# decision_function measure the distance of each test example from the hyperplane separating \n",
    "# the two classes. the sign of the value indicates which side of the hyperplane the prediction\n",
    "# for a sample is on\n",
    "\n",
    "y_score = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_score is a 1 * X_test.shape array. with a test size of .5 and a limit on the dataset such \n",
    "#that we only look at 2 of the three classes and only 100 of the 150 samples, that make y_score \n",
    "#a 1 by 50 array\n",
    "y_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20078869,  0.30423874,  0.20105976,  0.27523711,  0.42593404,\n",
       "       -0.15043726, -0.08794601, -0.12733462,  0.22931596, -0.23913518,\n",
       "       -0.06386267, -0.14958466, -0.04914839,  0.09898417,  0.0515638 ,\n",
       "       -0.1142941 ,  0.18899737,  0.04871897, -0.08258102, -0.26105668,\n",
       "        0.24693291, -0.18318328, -0.38384994,  0.26336904,  0.12585371,\n",
       "       -0.03991278,  0.39424539,  0.42411536, -0.4790443 , -0.30529061,\n",
       "       -0.09281931,  0.01213433, -0.20204098,  0.40148935, -0.04536122,\n",
       "        0.12179099,  0.06493837, -0.07007139,  0.0032915 , -0.39635676,\n",
       "        0.02619439,  0.20018683,  0.065023  ,  0.49589616, -0.28221895,\n",
       "        0.31364573,  0.1906223 ,  0.11549516,  0.03145977,  0.22408591])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each number corresponds to the distance that the test example is from the hyperplane. the \n",
    "# bigger the number the further away it is\n",
    "# the sign indicates which side of the hyperplane it's on\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a 1 or a 0 to indicate which class the classifier believes each training example belongs\n",
    "\n",
    "y_predict = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class predictions. the 0's have the same index as the negative numbers in the y_score variable\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  1,  0,  0,  1,\n",
       "        1, -1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict - y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the average precision score, Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.88\n"
     ]
    }
   ],
   "source": [
    "# this compares the y_score variables with the true y_test labels and returns the recall value\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
