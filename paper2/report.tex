\documentclass[sigconf]{acmart}


\input{format/i523}


\begin{document}
\title{Big Data Applications in Using Neural Networks for Medical Image Analysis}


\author{Tyler Peterson}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University - School of Informatics, Computing, and Engineering}
  \streetaddress{711 N. Park Avenue}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{typeter@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}

 Medical image analysis is proving to be a promising domain for disruption by machine learning. The analysis of medical imagery has long been within the purview of radiologists, a specialization in medicine that reviews medical imaging to form diagnoses and advise on treatment options. Historically, radiologists have relied on their training, senses and years of experience to evaluate images for medical issues, such as the presence of tumors, lung nodules, and hip osteoarthritis. The presence of technology, generally referred to as computer-aided diagnosis (CAD) tools, has been growing over the last several decades, but modern computing power and sizable datasets have accelerated the effectiveness of these assistive tools. Machine learning algorithms, especially artifical neural networks (ANN), are being leveraged to help identify problems present in medical images at a high level of accuracy. Several research studies conclude that ANN techniques can match, and occassionally outperform, the abilities of radiologists. Big data and the application of advanced algorithms show promise for evolving our ability to successfully evaluate medical images and save lives in the process.      
  
\end{abstract}


\keywords{i523, hid331, Big Data, Medical Image Analysis, Artificial Neural Networks, Medicine}


\maketitle


\section{Introduction}

The analysis of medical imagery is primarily the responsibility of radiologists. These individuals are medical doctors who specialize in diagnosing diseases through review of images produced by various imaging modalities, such as x-ray, ultrasound, computerized tomography (CT), magnetic resonance imaging (MRI) and positron emission tomography (PET) \cite{cite00}. Radiologists serve as an expert to other physicians by analyzing the medical images of patients suspected of having certain medical issues, and by making recommendations on subsequent care based on the observations \cite{cite00}. The images reviewed by radiologists are generally stored digitially, and images are increasingly being stored in picture archiving and communications systems (PACS). These systems are expected to keep up with the rapid accumulation of medical image data. Between 2005 and 2011, the medical image data in US hospitals increased from only 8,900 terabytes to 27,000 terabytes \cite{cite04}. That number is expected to grow by 20 percent every year due to increasing image size, the adoption of 3D imaging, and an aging population who will likely bring an increasing demand for medical imaging studies \cite{cite04}.

It is estimated that one billion medical images are created worldwide each year, and most of these are assessed by radiologists \cite{cite01}. Given that radiologists are human, their judgment is fallible. It is estimated that the lowest average error rate in analyzing medical imagery is 4 percent, which means collectively radiologists are estimated to make 40 million errors in judgement every year \cite{cite01}. A particularly striking example of fallibility comes from a study that analyzed the first and second interpretations of radiologists from Massachusetts General Hospital. They reviewed abdominal CTs and re-reviewed studies that had either been interpreted by themselves or a colleague. The study found that the radiologists disagreed with their peers 30 percent of the time and even disagreed with themselves 25 percent of the time \cite{cite01}.

There are two major types of radiologic analysis error: perceptual error and interpretive errors \cite{cite01}. Most errors, up to 80 percent, are perceptual errors, which occur when an abnormality is not perceived by the reviewer during the initial review, but is identified in a subsequent analysis \cite{cite01}. Interpretive errors occur when the radiologist successfully identifies the abnormality, but incorrectly diagnosis the problem, which may lead to a less appropriate course of careaction \cite{cite01}. There are several reasons why errors occur, including fatigue, excessive pace of analysis, distractions and insufficient knowledge of the practitioner. It is also asserted that the extreme complexity of a radiologist's job contributes to the errors. Errors occur in the practice of radiologists all across the world, at varying levels of training, in all imaging modalities and all clinical settings \cite{cite01}.

Over the last several decades, there has been an effort to develop computer-aided diagnosis (CAD) tools to help mitigate errors. These are systems that are intended to supplement, not replace, the radiologist by reporting a second opinion to be considered alongside the radiologist's assessment. The earliest initiatives to develop these tools occurred in the 1960s, and concerted efforts began in the 1980s \cite{cite02}. Despite the research being nearly 60 years old, widespread adoption is a relatively recent occurrence \cite{cite03}. Early implementations leveraged various machine learning algorithms to perform classification tasks around identifying the presence of abnormalities, but the effectiveness of these tools was reported as low in clinical studies. Specifically, CAD assessments included more false positives than human assessments, which led to additional, unnecessary medical tests and biopsies \cite{05}.

Several improvements in the field of computing have increased the accuracy of CAD tools and subsequently encouraged wider adoption of these tools into clinical workflows. The advancements includes increased access to digital imaging datasets, larger imaging data sets, increased used of imaging in healthcare and increased computer power \cite{cite03}\cite{cite05}. These factors combine to create an ideal state for research related to artifical neural networks (ANN) and the implementation of tools that can rival, and even outperform, the assessment of highly trained radiologists.

\section{Artificial Neural Networks}

history of

simple applications - numbers

math used - sigmoid function - why what does this do

back propagation

black box

how compare to other classification algorithms.



In a field that relies heavily on the perception of the reviewer, the occurrence of error is not especially surprising. 

Describe the workflow of a radiologist. what is the history of the profession

what is the error rate of radiologists. burnout

machine learning - what is it. what kinds of problems can it solve

why is image recognition well suited for deep learning

how much data is there

what is deep learning, how does it compare with human brains

what is the history of neural networks, why is now the time that it's proliferating

difference between types on neural networks, which are best suited to the tast

how is this service being offered, which companies are selling, what are they doing

problems, controversies, how is it being accepted. differences in how images are taken, quality

applications of neural networks Applications â€“ hip ortho, diabetic retinopathy, mammogram, melanoma detection, lung nodules, myositis

The increased attention and effectiveness of these tools is brought on by several factors, including sufficient computing power and the availability of sufficiently large train sets needed by the ANN to understand patterns in the data \cite{editor00}.

\section{Neural Networks}

\section{Infrastructure}

big data computing, how long it takes to train on these problems. GPUs vs CPUs

\section{Conclusion}

 This is my conclusion.

 
\begin{acks}

 These are my acknowledgements

\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 


\appendix
 

\input{issues}


\end{document}
