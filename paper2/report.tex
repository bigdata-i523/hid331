\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Big Data Applications in Using Neural Networks for Medical Image Analysis}

\author{Tyler Peterson}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University - School of Informatics, Computing, and Engineering}
  \streetaddress{711 N. Park Avenue}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{typeter@iu.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}

\begin{abstract}
  
  Medical image analysis is proving to be a promising domain for disruption by machine learning. The analysis of medical images has long been within the purview of radiologists, a specialization in medicine that reviews medical imaging to form diagnoses and advise on treatment options. Historically, radiologists have relied on their training, senses and years of experience to evaluate images for medical issues, such as the presence of malignant growths, lung nodules, and hip osteoarthritis. The use of technology, generally referred to as computer-aided diagnosis (CAD) tools, has been growing over the last several decades, but modern computing power and sizable datasets have accelerated the effectiveness of these tools. Machine learning algorithms, especially artifical neural networks (ANN), are being leveraged to help identify abnormalities present in medical images at a high level of accuracy. Several research studies conclude that ANN techniques can match, and often greatly improve, the abilities of radiologists. Big data and the application of advanced algorithms show promise for evolving our ability to successfully evaluate medical images and save lives in the process.
\end{abstract}

\keywords{i523, hid331, Big Data, Medical Image Analysis, Artificial Neural Networks, Medicine}

\maketitle

\section{Introduction}

The analysis of medical images is primarily the responsibility of radiologists. These individuals are medical doctors who specialize in diagnosing diseases through review of images produced by various imaging modalities, such as x-ray, ultrasound, computerized tomography (CT), magnetic resonance imaging (MRI) and positron emission tomography (PET) \cite{cite00}. Radiologists serve as an expert to other physicians by analyzing the medical images of patients suspected of having certain medical issues, and by making recommendations on subsequent care based on the observations \cite{cite00}. The images reviewed by radiologists are generally stored digitally, and images are increasingly being stored in picture archiving and communications systems (PACS). These systems are required to keep up with the rapid accumulation of medical image data. Between 2005 and 2011, the medical image data in US hospitals increased from only 8,900 terabytes to 27,000 terabytes \cite{cite04}. That number is expected to grow by 20 percent every year due to increasing image size and resolution, the adoption of 3D imaging, and an aging population who will likely bring an increasing demand for medical imaging studies \cite{cite04}.

It is estimated that one billion medical images are created worldwide each year, and most of these are assessed by radiologists \cite{cite01}. Given that radiologists are human, their judgment is fallible. It is estimated that the lowest average error rate in analyzing medical images is 4 percent, which means collectively radiologists are estimated to make 40 million errors in judgement every year \cite{cite01}. A particularly striking example of fallibility comes from a study that analyzed the first and second interpretations of radiologists from Massachusetts General Hospital. They reviewed abdominal CTs and re-reviewed studies that had either been interpreted by themselves or a colleague. The study found that the radiologists disagreed with their peers 30 percent of the time and even disagreed with themselves 25 percent of the time \cite{cite01}.

There are two major types of radiologic analysis error: perceptual error and interpretive error \cite{cite01}. Most errors, up to 80 percent, are perceptual errors, which occur when an abnormality is not perceived by the reviewer during the initial review, but is identified in a subsequent analysis \cite{cite01}. Interpretive errors occur when the radiologist successfully identifies the abnormality, but incorrectly diagnoses the problem, which may lead to a less appropriate course of care \cite{cite01}. There are several reasons why errors occur, including fatigue, excessive pace of analysis, distractions and insufficient knowledge of the practitioner. It is also asserted that the extreme complexity of a radiologist's job contributes to the errors. Errors occur in the practice of radiologists all across the world, at varying levels of training, in all imaging modalities and all clinical settings \cite{cite01}.

Over the last several decades, there has been an effort to develop computer-based tools to aid radiologists in the detection of abnormalities. Computer-aided diagnosis (CAD) tools are primarily intended to increase the rate at which problems are identified while also reducing the false negatives resulting from human error \cite{cite06}. These systems are intended to supplement, not replace, the radiologist by reporting a second opinion to be considered alongside the radiologist's assessment. The earliest initiatives to develop these tools occurred in the 1960s, and concerted efforts began in the 1980s \cite{cite02}. Despite the research being nearly 60 years old, widespread adoption is a relatively recent occurrence \cite{cite03}. Clinical studies reported early CAD implementations as being minimally effective. Specifically, CAD decisions included more false positives than human assessments, which created more work for radiologists and often led to additional, unnecessary medical tests and biopsies \cite{cite05}.

Several improvements in the field of computing have increased the accuracy of CAD tools and subsequently encouraged wider adoption of these tools into clinical workflows. The advancements includes increased access to digital imaging datasets, larger imaging datasets, increased used of imaging in healthcare and increased computing power \cite{cite03}\cite{cite05}. These factors combine to create an ideal state for research related to advanced machine learning techniques, namely artifical neural networks (ANN), and the implementation of tools that can rival the assessment of highly trained radiologists.

\section{Machine Learning and Artificial Neural Networks}

Broadly speaking, machine learning is a way of applying artificial intelligence to a problem through the analysis of data. Machine learning techniques assess the features, or attributes, of samples in a dataset to identify patterns in the data, and the resulting algorithm can be used to render conclusions about new inputs without human intervention \cite{cite05}. The ideal algorithm is represented by an equation that minimizes the error, or cost, made by the predictions. Medical image analysis presents what is referred to as a classification problem. The typical example of a classifcation problem is handwritten numerical digit recognition. In this example, a handwritten digit between 0 and 9 is fed into the algorithm, and the algorithm decides which of the ten digits, or classes, that handwritten digit is most likely to belong. Specific to medical image analysis, the classifier detects abnormalities in images otherwise not present in images of the same area in healthy individuals, and renders a conclusion as to what that abnormality is.

There are several different machine learning techniques that have traditionally been used in classification problems, such as support vector machines (SVM). SVMs are an example of a supervised machine learning model, meaning this method uses labeled data. Every sample in the dataset includes the label, or correct answer, along with a value for each of the attributes. The SVM identifies patterns in the labeled samples to create an algorithm, and new, unlabeled samples can be processed by the algorithm and given a prediction. In the task of medical image analysis, attributes have historically been identified and designed by human experts \cite{cite06}. For example, an expert would identify abnormalities by explicitly describing shape, texture, position and orientation of the abnormal biological structure \cite{cite07}.  In addition to this being labor intensive, the image features are specific to the immediate problem being explored and cannot be expected to work well for other image types \cite{cite06}.

ANNs are another class of machine learning that is increasingly being leveraged to tackle problems related to image analysis. ANNs, just like SVMs, are often utilized in a supervised manner, but do not require the painstaking process of expert-defined key attributes. Instead, ANNs learn the important features from the images themselves \cite{cite07}. This is an obvious advantage when compared to traditional machine learning methods, but the complexity of these algorithms has prevented the achievment of mainstream use until recently. The theory of ANNs was introduced in the 1950s, and research in this field has begun to flourish recently due to the increase in computing power and availability of high quality datasets \cite{cite05}.

\section{Overview of ANNs}

The inspiration for the design of ANNs is the biological neural network, more commonly known as the brain. The process by which data is input into the model, analyzed, and given an output is intended to mimic the way a brain absorbs and processes information before finally coming to a conclusion. In the brain, each neuron is capable of receiving multiple input signals and transmitting those signals via  synapses to other neurons \cite{cite05}. Similar to the brain, ANNs are made up of artificial neurons that take in inputs, or attributes, from the samples of the dataset. In the context of medical imaging, the inputs are numerical representations of each individual pixel of the original image. And just as neurons in the brain transmit information to other neurons via synapses, the artificial neurons pass information via artificial synapses. Each time information travels by way of artificial synapses, that information is multiplied by a weight \cite{cite05}. As with traditional machine learning techniques, the weights are intended to minimize the error, or cost, of the function, thereby returning a higher accuracy rate. The learning process of the ANN is driven by the adjustment of those weights, similar to how the neurons in the brain use external stimuli to adjust and redistribute evaluations. The weights in the ANN algorithm are initially randomized and subsequently adjusted by an optimization algorithm such as gradient decent, which guides the weights in a direction that minimize the cost of the function \cite{cite05}. 

There are several types of ANNs, and the types can be identified by the structure. The most basic neural network, called the perceptron, was initially theorized in 1957 and consisted only of an input layer and an outputer layer. This design limited the perceptron's problem solving ability to datasets that could be linearly separated \cite{cite08}. This is clearly not helpful for problems such as medical image analysis where the data presents complicated patterns and relationships.

In 1982, the Hopfield network was theorized, which adds a hidden layer of artificial neurons between the input and output layers \cite{cite08}. Its referred to as a hidden layer because the value of the neurons in the hidden layer does not correspond to a specific input value or a output class prediction \cite{cite05}. Each input neuron is connected to each neuron in the hidden layer, and each neuron in the hidden layer is also connected to each neuron in the output layer. It is important to note that while all neurons in neighboring layers are conncected to each other, the neurons within a single layer are not connected to each other \cite{cite06}. The benefit of the additional layers is that it allows the neural networks to combine numerous simple decisions to make more complicated decisions \cite{cite05}. Deep neural networks (DNN) are type of ANN that make use of several hidden layers between the input and output layers, and have demonstrated the capability of making complex decisions.

Convolutional neural networks (CNN) are an advanced type of ANN that are well suited to solving problems related to images. The fact that neighboring pixels are directly next to each other or near each other is an important piece of information that can tend to be lost by other types of ANNs that vectorize input values. CNNs, on the other hand, input images in a more direct and complete manner \cite{cite06}. CNNs are comprised of several types of layers, including convolutional, pooling and fully connected layers. Convolutional layers detect distinctive edges, lines and other perceptable visual features. This is intended to mimic how the brain perceives objects by observing distinct visual features \cite{cite05}. Pooling layers get that name because they pool together the image in a way that reduces the dimensions of the input sample while preserving the important details identified in the convolutional layer. Convolutional and pooling layers are often repeated several times before arriving at a fully connected layer, which ingregrates the results from the previous convolutional and pooling layers \cite{cite05}.

Several statistics can be used for evaluating the accuracy of ANNs. Sensitivity is the true positive detection rate. This is the percentage of positive occurrences that are successfully identified \cite{cite11}. A false positive may lead to unnecessary testing, unecessary expenses and unecessary stress on the patient who has been led to believe they have a certain condition. Specificity is the true negative detection. This is the percentage of negative occurrences that are successfully identified \cite{cite11}. A false negative may lead to a missed diagnosis, resulting in delayed treatment and a potentially avoidable death in some cases. Sensitivity and specificity can be assessed together by the receiver operating characteristic (ROC) curve. The ROC curve plots the true positive rate against the false positive rate (100 minus the true negative rate) for varying decision thresholds. This illustrates the trade-off between sensitivity and specificity and can provide guidance on which decision threshold is appropirate for the task \cite{cite11}. ROC curves are often leveraged to evaluate the performance of ANNs by calculating the area under the ROC curve, also known as the AUC. The goal is the maximize the AUC value, and that value points to the optimal balance between sensitivity and specificity \cite{cite11}.

\section{Applications in Medical Image Analysis}

There are several specialities in which medical image analysis has been studied and applied for the purpose of computer-aided detection and diagnosis, and the effectiveness of ANNs has been formally studied in different ways. This includes ANNs versus medical professionals, ANNs combined with medical professionals versus ANNs and medical professionals separately, and one type of ANN versus another type of ANN.

One study applied deep learning techniques to images of breast sentinel lymph nodes and evaluated the images for the presence of metastasis. A positive test likely means the staging of the breast cancer will be higher and subsequent treatment will be more aggressive. A false negative means a patient's disease will be thought of as less advanced than it actually is, and subsequent treatment will not be as aggressive as necessary. The medical professionals in this study obtained an AUC of 0.966, and the algorithm received an AUC of 0.925. Decisions made by a human also using the algorithmic conclusion as a second opinion achieved an AUC of 0.995, which equals an 85 percent reduction in error for the medical professional \cite{cite09}.

A second study compare the performance of three different types of ANNs on the detection of cancerous lung nodules in CT images. Lung cancer is a disease that greatly benefits from early detection. Over 220,000 new cases were identified in 2015, and an early detection of the disease improves the 5 year survival rate by roughly 50 percent \cite{cite10}. CT images provide three-dimensional (3D) views of the chest and are a key component of the clinical workflow of this specialty. These image are analyzed to understand if the structures in the image are part of the expected anatomy or if the structure is a tumor. If a tumor is present, the goal is to understand if the nodule is benign or malignant. This determination closely depends on the size, shape and texture of the nodule, all of which can be analyzed by an ANN. This study compared the performance of a DNN, a CNN and another type of ANN called a stacked auto-encoder (SAE). The CNN performed best with an AUC of 0.916, while the DNN and SAE recorded AUCs of 0.877 and 0.884 respectively \cite{cite10}.

A third study compares the performance of an ANN to two experienced physicians in the evaluation of x-rays for the presence of hip osteoarthritis. Hip osteoarthritis causes pain and stiffness which can diminish quality of life through an inability to perform daily tasks or go to work \cite{cite12}. Hip osteoarthritis is diagnosed through x-ray imaging studies, which are traditionally evaluated by radiologists. The time consuming and error-prone nature of this work is well documented, and with an increasingly aging population, efficient and timely diagnosis of hip osteoarthritis is growing in importance. The study used a CNN to achieve a sensitivity rate of 95.0 percent and a specificity rate of 90.7 percent, with an AUC of 0.94. Both physicians achieved a sensitivity of 100 percent and specificity rates of 86.0 percent and 93.0 percent. While the CNN recorded slightly lower sensitivty compared to the two physicians, it did record a higher specificity than one of the physicians \cite{cite12}. This shows promise for the performance of ANNs in evaluating hip osteoarthritis.

\section{Infrastructure}

Optimization of an ANNs can require billions of calculations, if not more, and the task therefore requires special hardware to help accomplish the task. At a very high level, there are two tasks involved in training a model. First, the input data is passed forward through the neurons which provides an output and an accompanying error rate. Second, with the error rate in hand, the weights of each synapse in the model are adjusted with the goal of lowering the error rate. This process is repeated many, many times. A common deep learning deployment called VGG16 has 16 hidden layers and roughly 140 million parameters \cite{cite14}. At each point in the network the computer must complete a matrix multiplication task, and the sheer volume of calculations means the task will take a significant amount of time to complete \cite{cite14}\cite{cite13}.

Graphical proccessing units (GPUs) are better suited for this task than central processing units (CPUs). The key difference between GPUs and CPUs is that GPUs can parallelize the matrix operations necessary to train the model, whereas CPUs are far less able to do so. CPUs typically only have a handful of cores, while GPUs can contain hundreds, if not thousands \cite{cite13}. GPUs can perform several matrix operations at once and CPUs need to perform those sameoperations one at a time. For example, training a CNN with four hidden layers takes 8000 seconds with a CPU and only 1000 seconds with a GPU \cite{cite13}.

This foundation has led to the creation of GPU-based super computers. The Commonwealth Scientific and Industrial Research Organization (CSIRO) acquired a new super computer made by Dell in 2017. Part of the infrastructure includes 114 PowerEdge C4130 server with Nvidia Tesla P100 GPUs, which includes over a million computing cores and 29TB of RAM \cite{cite15}.


\section{Challenges}

There are several challenges that may inhibit the success of widespread use of CAD tools built upon a neural network algorithm. First, overfitting occurs when an algorithm is trained based on a dataset that does not generalize well to examples outside of the data used to train the algorithm. Many studies demonstrating the value of ANNs were trained using relatively small datasets, and because the significant features present in a small dataset may not be the  same features present in a large dataset, the algorithm derived from the small dataset may not perform well when analyzing images from the large dataset \cite{cite08}\cite{cite05}. This issue can be addressed by training algorithms on larger datasets, but of course requires access to larger datasets, longer training periods and more computing power.

Second, algorithms derived from ANNs are frequently considered to be black boxes, meaning that it is nearly impossible to understand how the algorithm reached a certain conclusion. This contrasts with several other types of machine learning techniques that produce equations that highlight which features are significant \cite{cite05}. Instilling belief and trust in a system that is difficult, if not impossible, to explain is a barrier, even if that system produces accurate responses the vast majority of time.

Third, ethical and legal considerations must be made. Adopters of this technology must consider scenarios where the system makes a prediction that harms a patient \cite{cite05}. If a radiologist is led to a conclusion by an algorithm, and the algorithm presents a false positive, a false negative or presents one conclusion while missing another, who is responsible for the error?

Fourth, the ANNs are dependent on the quality and nature of the imaging data used to train algorithms. There is variability around the world in regards to the type and quality of imaging machines and the imaging protocols that dictate why and how images are taken \cite{cite05}. Two different imaging machines taking a picture of the same body site may produce meaningfully different images. Further, two technicians may use the same machine differently when imaging the same body site, and this may also produce meaningfully different images. It is conceivable that these images could appear different to an algorithm to the extent that the prediction is not the same. This issue could at least partially be addressed by sufficiently large datasets containing labeled images of abnormalities that were taken using machines of varying quality and executed using differing methods.

\section{Conclusion}

ANNs represent a great step forward in our ability to program computers to rapidly evaluate information in a manner similar to that of human experts. The process demonstrates great capabilities in learning important features programmatically, as opposed to researchers needing to consult with experts to handcraft meaningful features. Widespread adoption of this technology is beginning to pick up speed as the accuracy of these algorithms approaches that of experts. While research does not yet conclusively indicate that algorithms can independently outperform experts, at the very least the combination of an expert and a modern CAD tool frequently leads to higher accuracy compared to the expert operating alone. Continued advances in computing technology and the accumulation of larger and larger imaging datasets will likely further increase the power of these tools.

\begin{acks}

Thank you to Gregor von Laszewski and his teaching assistants for their help with the classes numerous questions and concerns.
  
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\appendix
 
\input{issues}

\end{document}
